Kernel Principle Component Analysis
============================

Summary
-------

Kernel Principal Component Analysis (kernel PCA) extends `principal component analysis (PCA) <PCA.html>`_ using kernel methods. KPCA allows originally linear operations of PCA to performed in a generalizes euclidean space ([Reproducing kernel Hilbert space](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)).

Given an appropriate kernel, it is possible to construct a hyper plane that linearly separates the classes the in feature space. 

.. figure:: ../_static/tutorials/kpca1.png
    :align: center

    Left: Input points before KPCA. Center: Output after kernel PCA with :math:`k(\mathbf{x}^{\top}\mathbf{y}+1)^2`. Right : Output after KPCA with a Gaussian kernel. (`source <https://en.wikipedia.org/wiki/Kernel_principal_component_analysis>`_)

Theory
------

In general, kernel methods use a non-linear transformation :math:`\phi: \mathbb{R}^d \to \mathbb{R}^'` to handle data :math:`mathbb{X} = [\mathbf{x}^{(1)}, .. \mathbf{x}^{(N)}]` in feature space. 

Kernel Trick
~~~~~~~~~~~~

Calculating directly in feature space is unrealistic (*exaplanation*). However, by defining a kernel function :math:`k(\mathbf{x}^(i), \mathbf{x}^(j))=\phi(\mathbf{x}^(i))^{\top}, \phi(\mathbf{x}^(j))` to calcualte the inner product we can perform inner product calculation in the original space. Therefore greatly reducing calculation time. This approach is named the kernel trick.

Radial Basis Function Kernel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In many kernel based classification methods, the Radial Basis Function Kernel is used. It is defined as the following:

.. math::
    k(\mathbf{x}^(i), \mathbf{x}^(j)) = exp(-\frac{||\mathbf{x}^(i)-\mathbf{x}^(j)||^2{2/sigma ^2}})

.. note::
    Subspaces methods such as KMSM and KCMSM universally use RBF as the kernel. RBF is also the default parameter for in support vector classifier in the `scikit-learn implementation <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>`_ .

Gaussian Kernel
~~~~~~~~~~~~~~~
Calculation of PCA
------------------

Here, we begin by explaining the usual PCA used in Quadratic Discriminant Function (QDF). This follows the introduction of that in :cite:`maeda2010subspace`.

Let, :math:`x_i` be the :math:`i^{th}` vector in our target dataset with :math:`N` samples, and :math:`\mu` be the mean :math:`\frac{1}{N}\sum_{i}x_i`. :math:`\phi` is the unit vector that represents the direction that maximizes the variance of the distribution.

Our goal is to capture the direction that maximizes the variance of the distribution, e.g. solve for :math:`\phi` with the condition that :math:`|| \phi ||=1`. This can be formulated with the introduction of an unknown multiplier :math:`\lambda` and follows:

.. math::
    J = \sum{i} (x_a - \mu, \phi)^2 - \lambda (||\phi||^2-1) \\
    = \sum_{i}\{ \sum_{j}(x_{ij} - m_j), \phi \}^2 - \lambda (\sum_{j}||\phi||^2-1)

We can solve for :math:`|| \phi ||` by setting the partial derivatives of J to zero. This leaves us with:

.. math::
    \sum_{i}(x_i - \mu, \phi)(x_i - \mu) - \lambda (2\phi) = 0

This can be rewritten further as shown below,

.. math::
    \sum_{i}(x_i - \mu)(x_i - \mu)T\phi = \lambda (2\phi)
    
If we focus on :math:`\sum_{i}(x_i - \mu)(x_i - \mu)`, we can see this is a matrix. Replacing this matrix with :math:`K` such that :math:`K=\sum_{i}(x_i - \mu)(x_i - \mu)`, we find a formula that many will find familiar.

.. math::
    K \phi = \lambda\phi

In conclusion, \phi becomes the eigen vector of covariance matrix K. As for :math:`\phi(i>0)`, the story is the same and :math:`\phi` are obtain as the eigenvectors.

.. figure:: ../_static/pca/pca.gif
    :align: center

    Code can be found in the `gallery <../examples_scripts/plot_pca.html#sphx-glr-examples-scripts-plot-pca-py>`_.

